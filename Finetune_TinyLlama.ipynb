{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMeTHGLmno4ERgW0z749c63",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madarasw/Machine-Learning/blob/main/Finetune_TinyLlama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1fG3PQhdiHT",
        "outputId": "032abeb7-ebc1-442b-adc7-a791ddaefc77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers peft accelerate datasets bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True  # reduces memory use (QLoRA style)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383,
          "referenced_widgets": [
            "3d231784f5514451ba37e27b5407e2d8",
            "bd77dd6fa3eb46289fe3372f4b13185d",
            "16d09be336504bc88ac4a64a721c62cf",
            "51207a2f791d47548096d1e111523b9f",
            "7704aeae0573456ea36e04e8984712b9",
            "916bee95b6d44e60ac99dbeb13957fdc",
            "679019352fc947a0bf0b2a122d91bdb6",
            "0a0432bc5f3f4d3ab4bf6908fe183a9e",
            "930ed8bbdd7c4460881d290126fc0f24",
            "79e918bdd3e042f49749724a6717d24f",
            "07566e1412b54fb6970d914b88e79ca9",
            "ba4c7ba217744b7abcaa2bf39f1ab774",
            "e1d4ee34fdba404a98fb2411e41781de",
            "a66b239012184d07bfa4c8f67dfd956a",
            "22c6556a265b4ef79ba4be367f40866a",
            "789984aa26d646afaed85415b0888789",
            "30b5f7b731d34feaa5e89cabdfeb9ca4",
            "a3fbd6bb6df346a890908132ff637b50",
            "d8151b4860444fedb81b3e3b2fc14d33",
            "337849dd6db94a3a8f51b78fbf83542f",
            "fc6955db0c644359b9b5be6d624c90a6",
            "20e2808dd7c146f39f8c3c25423f9104",
            "29e86ad3fa9d4071a3ed81fc10b90bd4",
            "1e27a78191bd4568950c5acd292b8032",
            "a70588c501634cecb47d632a66b97026",
            "6a37edd9e1954b19ba921cb5ff41eccb",
            "14d41cae221149218e7d317e57030b0b",
            "6b9ae06b93d44516aceb4d6e0ccbc8f8",
            "c2c8c30d151e4086af6bcbd9f221f30d",
            "638f9ef8c3494743b4f6b667fbbe4397",
            "5a669412a50e482591c1313aa13bc343",
            "7e00272c5ebc4b78aac0bf03b1251519",
            "2dbb6ccd02ed41168faa66fb2a318c73",
            "b0aaa2d54929484daa87c24cd877aa35",
            "d7b6003967b84448af3d890796b78c27",
            "e8f165e5b4524d8ba002d7dd35beae7f",
            "c5a70c17842e47ffbf2f807579fc482b",
            "fd077f799b344e20ba21e9b938c19a80",
            "78a16f09abdb4ddca0c46f76c1eea539",
            "076c18c255364f5aa4e92590c1628c7b",
            "b3fba010b95643fb9f22f1db67fb9e46",
            "79fd49dc885748ec93411db9fa9eba4c",
            "910e0d6011064d65bf3f3e0480363821",
            "74fd728902e748c78a54368abdd861b6",
            "eb9dd02b2222481c8819bcf28ae359ed",
            "40e14b3db63f439f889bc8342e22a0be",
            "af2fc100b9f640bea081d6df2797049e",
            "5178f8915c9f48528931cc1e271c8471",
            "33edda5c3c2342a6b3167f7b14d40cbd",
            "9af5c7e7bded4a5a887f572dc0f39aad",
            "7536a202a2634b00ba9e083f2b4c2d8c",
            "ae97e1a95fbf4327b6e0fbf751c3df45",
            "5b022a2675644128ab336b43ec114af9",
            "2911f4d1d96c4990b51e277bac215c28",
            "4017c1bc1fb74224af362ea73d2fa1e4",
            "128119d253ba4ca6a2773ef8fa9c9545",
            "78c07aa07bc54b64a17736018a0b2b62",
            "aae24b13c8ba439f9fa4b8f7070b148b",
            "b6cd746e06a8411392e56a37d820075b",
            "f3a24b474d824265bda093b9f2c55031",
            "b60809f5924a476f979a7e2e3250f8a1",
            "7bbec0d4704f42f0875fb0e2070e6aea",
            "e466a469627c494fb11d51b83e146685",
            "c0a3ecce5e4b428392d590dbf31e6016",
            "30aa6f7d82534952ad950421404c3f1a",
            "b5463051eb054fcdb294d859bfd08da6",
            "e9978ef8b7bf428598dfed7abe2a2aca",
            "21646b1847bf4f30be270f48d8dbefb7",
            "8aa9dbc618dd4ca2812ab3dae1c87c86",
            "bf3c76cc649744e08ec488931bd4770b",
            "77d135631bd44478b6524b8dd3bc9002",
            "c5230bc2964d426aaae3c8191ce82ae9",
            "0aef82652f75489db075a96bb819646d",
            "e66f8530c8eb492395a7844363e64ec8",
            "a3460d4aced146a4b30197179c59e609",
            "bf0e49d0db8548e8add12c01dda0631c",
            "031ffb52ea3644b0a0cda2d506459ea6"
          ]
        },
        "id": "OghT-4iGdvPH",
        "outputId": "38ba7d13-99f7-4a73-d8bc-46491c6b43a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d231784f5514451ba37e27b5407e2d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba4c7ba217744b7abcaa2bf39f1ab774"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29e86ad3fa9d4071a3ed81fc10b90bd4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0aaa2d54929484daa87c24cd877aa35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb9dd02b2222481c8819bcf28ae359ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "128119d253ba4ca6a2773ef8fa9c9545"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9978ef8b7bf428598dfed7abe2a2aca"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "data = {\n",
        "    \"instruction\": [\n",
        "        \"What is inflation?\",\n",
        "        \"Explain GDP in simple terms.\",\n",
        "        \"What causes unemployment?\"\n",
        "    ],\n",
        "    \"response\": [\n",
        "        \"Inflation is the general increase in prices over time.\",\n",
        "        \"GDP is the total value of goods and services produced in a country.\",\n",
        "        \"Unemployment happens when people who want to work cannot find jobs.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data)"
      ],
      "metadata": {
        "id": "dM1gBXkmd5u0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format(example):\n",
        "    return {\n",
        "        \"text\": f\"### Instruction:\\n{example['instruction']}\\n### Response:\\n{example['response']}\"\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(format)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "89b016113e714132be71c2f076a92585",
            "dbefde8be0674e56b6a6c05931c50acd",
            "b489c3232d2d454d8aed5b8b8bdf2912",
            "0abdcab0584c44d283ca143e22e75ca2",
            "364a874242e34369a78db9c268e019eb",
            "dba91f9441cc48e38cc84f54754f5be5",
            "e8c582bf380c4c4ca78c1c118b3fec18",
            "5d5cba369a284c4fac7a62ea2b367970",
            "04802aa1000649e2aede0cc7e784bdee",
            "90c7264a06974a8ab7748391938f73a8",
            "b0e220bac37c4cb9aa4988326dda33b5"
          ]
        },
        "id": "V82hHezxeOQw",
        "outputId": "29cc4d6d-549c-4ad5-ea1e-f19888539a8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89b016113e714132be71c2f076a92585"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "JnLFfNHneXWM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format(example):\n",
        "    return {\n",
        "        \"text\": f\"### Instruction:\\n{example['instruction']}\\n### Response:\\n{example['response']}\"\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(format)\n",
        "\n",
        "def tokenize(example):\n",
        "    outputs = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()  # ✅ add labels for causal LM\n",
        "    return outputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "fa4c2dda0f97494390c04e78d15fac6d",
            "b85289afad4f49cdb9a209f66c5b2242",
            "8292cc088f984f5898fbcf255937e654",
            "7df2740d88f548809b21c2cddd22dbf4",
            "f2eb002b5bad454193c2a4a27a759109",
            "23b1588d16bf4128924da728aab40c52",
            "5b0aebf4891f4fa78253b0accddf7475",
            "c64cd6f9a1c04b3fbe94460e0b8adcbe",
            "163347874d984d2c9ef14af92fd77d30",
            "d0567207dea34cec977b15d4e27e644d",
            "cc1e992bf71644109822e0ed0ff31cd7",
            "34c5b47b6a784fdf88fb7cfedcbc5870",
            "1363b2e14cc149d7b13c3369c760016f",
            "b3608e521e7b4eaaa6a5ff49c91a4f01",
            "09de0b4d967141eda51ef462bf021658",
            "60e4d3762a224ecc8f2d6d7c322b6346",
            "07da682510164cc7b465de54298a64ba",
            "c1f0a4283ca8496b8c3a604741e14a3b",
            "6867c6d00fd547598983d07ab4a538ed",
            "b8bd1255c555406b86fc2107e0b2d707",
            "615b20e936324f209fa4725de8e96fa2",
            "197796fb5b6d458883386361cc43a2e2"
          ]
        },
        "id": "4YB-PjOteah4",
        "outputId": "1c526a87-e493-4426-bb1e-12a877aac008"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa4c2dda0f97494390c04e78d15fac6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34c5b47b6a784fdf88fb7cfedcbc5870"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"tinyllama-lora\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=5,\n",
        "    remove_unused_columns=False,  # keep all custom fields\n",
        "    report_to=\"none\" # Disable wandb logging\n",
        ")"
      ],
      "metadata": {
        "id": "EoDjme03ee4G"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "CVWT04mYfDj6",
        "outputId": "11306d6a-4ccd-4f5d-f044-70dcec0b9ed6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:01, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2, training_loss=17.42969512939453, metrics={'train_runtime': 3.4481, 'train_samples_per_second': 1.74, 'train_steps_per_second': 0.58, 'total_flos': 19088894066688.0, 'train_loss': 17.42969512939453, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"tinyllama-lora\")\n",
        "tokenizer.save_pretrained(\"tinyllama-lora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b5nZkTWfZCj",
        "outputId": "5470dfc8-d2ee-41eb-9114-08e29f0e5be3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tinyllama-lora/tokenizer_config.json',\n",
              " 'tinyllama-lora/special_tokens_map.json',\n",
              " 'tinyllama-lora/chat_template.jinja',\n",
              " 'tinyllama-lora/tokenizer.model',\n",
              " 'tinyllama-lora/added_tokens.json',\n",
              " 'tinyllama-lora/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "prompt = \"### Instruction:\\nExplain GDP.\\n### Response:\\n\"\n",
        "print(pipe(prompt, max_new_tokens=50)[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b9mTGGVhTCu",
        "outputId": "65ee5268-49e3-4b43-f150-54876d14943e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Explain GDP.\n",
            "### Response:\n",
            "GDP (Gross Domestic Product) is the total value of all goods and services produced in a particular economy in a given year. It is a measure of the economic strength and productive capacity of a country. It reflects the economic performance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "prompt = \"### Instruction:\\nExplain CPI.\\n### Response:\\n\"\n",
        "print(pipe(prompt, max_new_tokens=50)[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB8-JojKhU5Y",
        "outputId": "a79cc6ba-9d19-4ffd-aac8-ad334448e474"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Explain CPI.\n",
            "### Response:\n",
            "CPI or Constant Pricing Index measures the average decrease in prices during a specific time period. It is calculated by dividing the number of sales during a specific time period by the average number of sales during the same period. This method helps to identify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "prompt = \"### Instruction:\\nExplain Madara.\\n### Response:\\n\"\n",
        "print(pipe(prompt, max_new_tokens=50)[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBU8TLOThdW8",
        "outputId": "9a382865-3112-470b-8563-ad8252e8dbfb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Explain Madara.\n",
            "### Response:\n",
            "Madara was a powerful and mysterious sorcerer who had been banished from the kingdom of Altsin. He had been accused of several crimes, including the assassination of the king, plotting against the kingdom's rul\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "prompt = \"### Instruction:\\nExplain GDP.\\n### Response:\\n\"\n",
        "print(pipe(prompt, max_new_tokens=50)[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "6v5bhqWChigd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
